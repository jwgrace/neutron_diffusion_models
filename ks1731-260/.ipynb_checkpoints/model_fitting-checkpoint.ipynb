{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649a5b44-a6ff-47fe-bb90-e28fb8b51f88",
   "metadata": {},
   "source": [
    "### KS 1731-260 (Traditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e97d5a-513c-4812-9233-347d7d37668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import emcee\n",
    "import corner\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set path to dStar directory to import reader for reading history.data files\n",
    "home_directory = os.getenv('HOME')\n",
    "dStar_directory = home_directory + '/dStar/'\n",
    "sys.path.append(dStar_directory + 'tools/')\n",
    "\n",
    "import reader as dStar_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c85cec-6247-4249-882e-5386195d66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability(parameter_values, \n",
    "                    parameter_minimum_values=np.array([8.35e7, 0.0, 0.0]), \n",
    "                    parameter_maximum_values=np.array([9.35e8, 20.0, 20.0])):\n",
    "    '''\n",
    "    Calculates the log of the probability of a model with dStar model \n",
    "    parameters for use in an MCMC.\n",
    "    \n",
    "    The log of the probability is based on the chi^2 error assuming a Gaussian\n",
    "    probability distribution: probability = exp(-chi2/2). I ignore leading\n",
    "    coefficients since the MCMC only uses ratios of probabilities so they would\n",
    "    cancel anyway.\n",
    "    \n",
    "    Uses uniform priors for all model parameters with given minimum values and \n",
    "    maximum values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    parameter_values: array-like'\n",
    "        Input array of parameters for dStar model.\n",
    "        Must modify the src/run.f file to accomodate wrappers for the chosen \n",
    "        parameters.\n",
    "        Parameters here are: [core_temperature, Qimp, Q_heating_shallow]\n",
    "        \n",
    "    parameter_minimum_values: array-like\n",
    "        Minimum values for each dStar model parameter.\n",
    "        If any parameter is less than its corresponding minimum value, this \n",
    "        function will return -np.inf (0 probability).\n",
    "        \n",
    "    parameter_maximum_values: array-like\n",
    "        Maximum values for each dStar model parameter.\n",
    "        If any parameter is greater than its corresponding minimum value, this \n",
    "        function will return -np.inf (0 probability).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    log_probability: float\n",
    "        The log of the probability for use in the MCMC sample selection \n",
    "        process.\n",
    "    \n",
    "    '''\n",
    "    # Check for any parameters less than the minimum values or greater than the \n",
    "    # maximum values.\n",
    "    # If any parameters are outside the allowed range, then return -np.inf.\n",
    "    if np.any(parameter_values < parameter_minimum_values) or \\\n",
    "        np.any(parameter_values > parameter_maximum_values):\n",
    "        return -np.inf\n",
    "    # If paramaters are within the allowed range, run the models and calculate \n",
    "    # the log of the probability.\n",
    "    else:\n",
    "        # Run the model and save the output chi2\n",
    "        # Convert all parameter values to strings so they can be used as input \n",
    "        # in the subprocess command.\n",
    "        core_temperature = str(parameter_values[0])\n",
    "        Qimp = str(parameter_values[1])\n",
    "        Q_heating_shallow = str(parameter_values[2])\n",
    "        # Try to run the dStar model which calculates and outputs the chi2.\n",
    "        # If the model can't run or if the chi2 is too high, it won't produce\n",
    "        # output that can be converted to a float and will return an error.\n",
    "        # In this case just return -np.inf (probability = 0).\n",
    "        try:\n",
    "            chi2 = float(subprocess.run(['./run_dStar', '-T', core_temperature, \n",
    "                                         '-Q', Qimp, '-H', Q_heating_shallow], \n",
    "                                        capture_output=True, text=True).stdout)\n",
    "            return -chi2/2\n",
    "        except:\n",
    "            return -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2518df-cd82-4556-8f4a-e98431bdc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LOGS_data(LOGS_directory):\n",
    "    \n",
    "    f = dStar_reader.dStarReader('{}/history.data'.format(LOGS_directory))\n",
    "\n",
    "    # time after outburts in days\n",
    "    t = f.data.time\n",
    "    # log(L) [ergs]\n",
    "    lg_L = f.data.lg_Lsurf\n",
    "    # log(T_eff) [K]\n",
    "    lg_Teff = f.data.lg_Teff\n",
    "    \n",
    "    # constants in cgs\n",
    "    c = 3e10\n",
    "    G = 6.67e-8\n",
    "    M_sun = 1.989e33\n",
    "\n",
    "    M = M_sun*f.header.total_mass\n",
    "    R = f.header.total_radius*1e5\n",
    "    redshift_factor = (1 - 2*G*M/(R*c**2))**(-.5)\n",
    "    \n",
    "    Teff_inf = 10**(lg_Teff)/redshift_factor\n",
    "    \n",
    "    return t, lg_L, lg_Teff, Teff_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236bb5f2-8f94-46c2-8364-d9b4b4e1a0df",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "\n",
    "Load observational data from file. Columns of data file are:\n",
    "observatory, time after outburst (days), kTeff (eV), kTeff_err (eV)\n",
    "\n",
    "dStar outputs temperatures in K, so need to convert observational data from eV to K for comparison when plotting.\n",
    "\n",
    "Set initial guesses for the MCMC.\n",
    "Here I use the best-fit values from Merritt et al. 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e072bdba-6091-4773-9b80-984919dbba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'observational_data-ks1731-260'\n",
    "\n",
    "observatory = np.loadtxt(data_file, skiprows=3, usecols=0, dtype=str)\n",
    "t_obs, Teff_obs, Teff_obs_sigma = np.loadtxt(data_file, skiprows=3, usecols=(1,2,3), unpack=True)\n",
    "\n",
    "# Boltzmann constant for converting temperature from eV to MK\n",
    "k_B = 8.617e-5\n",
    "# Convert temperature from eV to MK\n",
    "Teff_obs *= 10**-6/k_B\n",
    "Teff_obs_sigma *= 10**-6/k_B\n",
    "\n",
    "# Initial guesses for parameters: Core Temperature, Qimp, Q_heating_shallow\n",
    "# Start sampling from fit_lightcurve example in dStar directory\n",
    "parameter_values_initial = np.array([9.35e7, 4.2, 1.37])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e77d47e-bade-490c-8f49-336f140870dd",
   "metadata": {},
   "source": [
    "### MCMC\n",
    "\n",
    "I use the examples in https://emcee.readthedocs.io/en/stable/tutorials/parallel/ as a template for my MCMC. In particular I use the examples in the \"Parallelization\" and \"Saving and Monitoring Progress\" sections.\n",
    "\n",
    "It saves the progress periodically so that if the run ends early for any reason, it can be reloaded and continued. It also checks for convergence and will end once it converges. Here I set the convergence criterion to 100 times the autocorrelation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b4f83e-ea2d-4496-ba3e-ebabec85243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]/home/justin/anaconda3/lib/python3.8/site-packages/emcee/moves/red_blue.py:99: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lnpdiff = f + nlp - state.log_prob[j]\n",
      "  1%|â–                                     | 99/10000 [05:24<9:01:08,  3.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m old_tau \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Now we'll sample for up to max_n steps\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m sampler\u001b[38;5;241m.\u001b[39msample(pos, iterations\u001b[38;5;241m=\u001b[39mmax_n, progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Only check convergence every 100 steps\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sampler\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/emcee/ensemble.py:402\u001b[0m, in \u001b[0;36mEnsembleSampler.sample\u001b[0;34m(self, initial_state, log_prob0, rstate0, blobs0, iterations, tune, skip_initial_state_check, thin_by, thin, store, progress, progress_kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_moves, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Propose\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m state, accepted \u001b[38;5;241m=\u001b[39m \u001b[43mmove\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m state\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tune:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/emcee/moves/red_blue.py:93\u001b[0m, in \u001b[0;36mRedBlueMove.propose\u001b[0;34m(self, model, state)\u001b[0m\n\u001b[1;32m     90\u001b[0m q, factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_proposal(s, c, model\u001b[38;5;241m.\u001b[39mrandom)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Compute the lnprobs of the proposed position.\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m new_log_probs, new_blobs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_log_prob_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Loop over the walkers and update them accordingly.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (j, f, nlp) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mzip\u001b[39m(all_inds[S1], factors, new_log_probs)\n\u001b[1;32m     98\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/emcee/ensemble.py:489\u001b[0m, in \u001b[0;36mEnsembleSampler.compute_log_prob\u001b[0;34m(self, coords)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m         map_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m\n\u001b[0;32m--> 489\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mmap_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mfloat\u001b[39m(l[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the number of walkers, number of dimensions, and maximum walker steps.\n",
    "nwalkers = 32\n",
    "ndim = len(parameter_values_initial)\n",
    "max_n = 10000\n",
    "\n",
    "# 2D array of initial positions of the walkers.\n",
    "# Center the walkers on the initial guess and randomly vary each parameter by up to 10%.\n",
    "pos = parameter_values_initial + .1*parameter_values_initial*np.random.randn(nwalkers, ndim)\n",
    "\n",
    "# File to save MCMC sampler data to.\n",
    "h5_filename = 'sampler.h5'\n",
    "backend = emcee.backends.HDFBackend(h5_filename)\n",
    "backend.reset(nwalkers, ndim)\n",
    "\n",
    "with Pool() as pool:\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, pool=pool, backend=backend)\n",
    "    #sampler.run_mcmc(pos, max_steps, progress=True)\n",
    "\n",
    "    # This will be useful to testing convergence\n",
    "    old_tau = np.inf\n",
    "\n",
    "    # Now we'll sample for up to max_n steps\n",
    "    for sample in sampler.sample(pos, iterations=max_n, progress=True):\n",
    "        # Only check convergence every 100 steps\n",
    "        if sampler.iteration % 100:\n",
    "            continue\n",
    "\n",
    "        # Compute the autocorrelation time so far\n",
    "        # Using tol=0 means that we'll always get an estimate even\n",
    "        # if it isn't trustworthy\n",
    "        tau = sampler.get_autocorr_time(tol=0)\n",
    "\n",
    "        # Check convergence\n",
    "        converged = np.all(tau * 100 < sampler.iteration)\n",
    "        converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)\n",
    "        if converged:\n",
    "            break\n",
    "        old_tau = tau\n",
    "        \n",
    "tau = sampler.get_autocorr_time()\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba9214-6d60-48b7-ba9c-2a9e28a376d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a sampler and continue progress\n",
    "'''\n",
    "new_backend = emcee.backends.HDFBackend(h5_filename)\n",
    "print(\"Initial size: {0}\".format(new_backend.iteration))\n",
    "with Pool() as pool:\n",
    "    new_sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, pool=pool, backend=backend)\n",
    "    \n",
    "    new_sampler.run_mcmc(None, 100, progress=True)\n",
    "print(\"Final size: {0}\".format(new_backend.iteration))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd13dc-c798-4f00-aa0e-f22cd7bc65b8",
   "metadata": {},
   "source": [
    "### Analysis - Parameter Distributions\n",
    "\n",
    "Since the MCMC saves all the data to the backend ('sampler.h5'), I can just load it from here if the MCMC is already done (don't need to re-run the MCMC). After loading the sampler data, just need to set the burnin steps to discard and optionally thin the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d293e-981c-4c20-9d86-f26699cb6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_filename = 'sampler.h5'\n",
    "reader = emcee.backends.HDFBackend(h5_filename)\n",
    "\n",
    "tau = reader.get_autocorr_time()\n",
    "burnin = int(2 * np.max(tau))\n",
    "#thin = int(0.5 * np.min(tau))\n",
    "samples = reader.get_chain(discard=burnin, flat=True)\n",
    "log_prob_samples = reader.get_log_prob(discard=burnin, flat=True)\n",
    "log_prior_samples = reader.get_blobs(discard=burnin, flat=True)\n",
    "\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca68994-196b-4761-9f91-f987725a8e24",
   "metadata": {},
   "source": [
    "### Corner Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63df603-6f1d-4658-8f87-c2710a2c79c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_names = ['Core Temperature (K)', 'Impurity Parameter', 'Shallow Heating (MeV)']\n",
    "\n",
    "fig = corner.corner(samples, labels=parameter_names)\n",
    "fig.patch.set_facecolor('white')\n",
    "fig.set_dpi(200)\n",
    "\n",
    "fig.suptitle('KS 1701-260 (Traditional)', fontsize=18)\n",
    "fig.savefig('corner_plot-traditional')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6904c-d853-446e-8f19-6d1b901f601a",
   "metadata": {},
   "source": [
    "### Best-Fit Parameters\n",
    "\n",
    "I follow the example in https://emcee.readthedocs.io/en/stable/tutorials/line/ for displaying the best-fit parameters along with the +/- uncertainties. I also calculate the $\\chi^2$ and reduced $\\chi^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9134f8-feef-4a25-bf7c-2d66b57bc5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = len(parameter_names)\n",
    "\n",
    "# Names of parameters for Math display\n",
    "parameter_names_math = ['Core Temperature', 'Q_{imp}', 'Q_{shallow heating}']\n",
    "\n",
    "from IPython.display import display, Math\n",
    "\n",
    "# Arrays for the best-fit parameters from the MCMC as well as the upper and lower errors.\n",
    "parameter_values_mcmc = np.zeros_like(parameter_values_initial)\n",
    "err_low = np.zeros_like(parameter_values_mcmc)\n",
    "err_high = np.zeros_like(parameter_values_mcmc)\n",
    "\n",
    "for i in range(ndim):\n",
    "    mcmc = np.percentile(samples[:, i], [16, 50, 84])\n",
    "    parameter_values_mcmc[i] = mcmc[1]\n",
    "    err_low[i] = mcmc[1] - mcmc[0]\n",
    "    err_high[i] = mcmc[2] - mcmc[1]\n",
    "    \n",
    "    txt = \"\\mathrm{{{3}}} = {0:.3f}_{{-{1:.3f}}}^{{{2:.3f}}}\"\n",
    "    txt = txt.format(mcmc[1], err_low[i], err_high[i], parameter_names_math[i])\n",
    "    display(Math(txt))\n",
    "    \n",
    "chi2 = -2*log_probability(parameter_values_mcmc)\n",
    "print('chi2 =', chi2)\n",
    "\n",
    "degrees_of_freedom = len(t_obs) - ndim\n",
    "reduced_chi2 = chi2/degrees_of_freedom\n",
    "print(reduced_chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99189859-26b4-4b30-9ed8-507926905e51",
   "metadata": {},
   "source": [
    "### Plot of Best-Fit Parameter Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6544b-5e77-47f0-acc8-f81b3099bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dStar model with the MCMC best-fit parameters to plot with observational data.\n",
    "\n",
    "core_temperature = str(parameter_values_mcmc[0])\n",
    "Qimp = str(parameter_values_mcmc[1])\n",
    "Q_heating_shallow = str(parameter_values_mcmc[2])\n",
    "subprocess.run(['./run_dStar', '-T', core_temperature, '-Q', Qimp, '-H', Q_heating_shallow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812eb28f-ab57-4ef9-b978-190f95df7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_best_fit, lg_L_best_fit, lg_Teff_best_fit, Teff_inf_best_fit = load_LOGS_data('LOGS')\n",
    "Teff_inf_best_fit *= 1e-6\n",
    "\n",
    "# Plot lightcurves of models and observational data with error bars\n",
    "\n",
    "# Only plot data with positive time (after end of outburst)\n",
    "time_mask = [t_best_fit > 0.]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "fig.set_dpi(150)\n",
    "\n",
    "plt.loglog(t_best_fit[time_mask], Teff_inf_best_fit[time_mask], label='Best-Fit Values')\n",
    "plt.errorbar(t_obs, Teff_obs, yerr=Teff_obs_sigma, fmt='.', label='Observational Data')\n",
    "plt.xlabel('t (days)')\n",
    "plt.ylabel(r'T$_{eff}$ (MK)')\n",
    "plt.title('KS 1731-260 (Traditional)')\n",
    "plt.legend(loc='upper right', fontsize=6)\n",
    "plt.show()\n",
    "fig.savefig('best_fit_lightcurve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdebae-9c6f-406f-9a0b-27eeadabb347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
